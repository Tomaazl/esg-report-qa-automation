"""
ESG Answer Ranker
------------------

Lightweight utility to suggest answer candidates for ESG questionnaire questions.

Answer candidates are loaded from an existing JSON answer base generated by
`pdf-qa-generator`, not from hardcoded templates.

Usage:
    from esg_answer_ranker import ESGAnswerRanker

    ranker = ESGAnswerRanker(qa_pairs_path="/workspace/pdf-qa-generator/output/qa_pairs.json")
    suggestions = ranker.rank_answer_candidates_for_question(
        "Do you track Scope 1 and Scope 2 emissions?",
        top_k=3,
    )
    # suggestions -> List[Tuple[str, float]] with (answer_text, score)

No external dependencies; uses simple keyword overlap and text similarity.
"""

from __future__ import annotations

import json
import os
import re
from dataclasses import dataclass
from difflib import SequenceMatcher
from typing import Iterable, List, Sequence, Set, Tuple


_WORD_RE = re.compile(r"[A-Za-z0-9]+(?:['-][A-Za-z0-9]+)?")


def _tokenize(text: str) -> Set[str]:
    """Tokenize text into a set of lowercase word tokens."""
    if not text:
        return set()
    return {token.lower() for token in _WORD_RE.findall(text.lower())}


@dataclass(frozen=True)
class AnswerCandidate:
    label: str
    text: str
    keywords: Set[str]


def _build_candidate(label: str, text: str, keywords: Iterable[str]) -> AnswerCandidate:
    return AnswerCandidate(label=label, text=text.strip(), keywords={k.lower() for k in keywords})


_STOPWORDS = {
    "the", "a", "an", "and", "or", "of", "to", "in", "on", "for", "with", "by", "from",
    "at", "as", "is", "are", "was", "were", "be", "been", "being", "that", "this",
    "these", "those", "it", "its", "we", "our", "you", "your", "they", "their",
    "will", "can", "could", "should", "would", "may", "might", "do", "does", "did",
}


class ESGAnswerRanker:
    """Ranks ESG answer candidates for given questions using an existing JSON answer base.

    Scoring model:
      - Keyword overlap score = matched_keywords / max(1, num_keywords)
      - Surface text similarity = difflib.SequenceMatcher ratio between question and candidate text
      - Final score = 0.7 * overlap + 0.3 * similarity

    This is intentionally simple and dependency-free. Extend by customizing keywords or similarity.
    """

    def __init__(
        self,
        answer_bank: Sequence[AnswerCandidate] | None = None,
        qa_pairs_path: str | None = None,
        overlap_weight: float = 0.7,
        similarity_weight: float = 0.3,
    ) -> None:
        if overlap_weight < 0.0 or similarity_weight < 0.0:
            raise ValueError("Weights must be non-negative")
        weight_sum = overlap_weight + similarity_weight
        if weight_sum == 0.0:
            raise ValueError("At least one weight must be positive")
        # Normalize weights to sum to 1.0 for interpretability
        self.overlap_weight = overlap_weight / weight_sum
        self.similarity_weight = similarity_weight / weight_sum

        if answer_bank is not None:
            self.answer_bank = list(answer_bank)
        else:
            # Default to project JSON if not provided
            json_path = qa_pairs_path or "/workspace/pdf-qa-generator/output/qa_pairs.json"
            self.answer_bank = self._load_candidates_from_json(json_path)

    def rank_answer_candidates_for_question(
        self,
        question: str,
        top_k: int = 3,
        min_keywords_to_consider: int = 0,
    ) -> List[Tuple[str, float]]:
        """Return top-K answer candidates as (answer_text, score) for a single question."""
        question = (question or "").strip()
        if not question:
            return []

        question_tokens = _tokenize(question)

        scored: List[Tuple[AnswerCandidate, float]] = []
        for candidate in self.answer_bank:
            if len(candidate.keywords) < min_keywords_to_consider:
                continue
            overlap_score = self._keyword_overlap_score(question_tokens, candidate.keywords)
            similarity_score = self._string_similarity(question, candidate.text)
            final_score = (self.overlap_weight * overlap_score) + (
                self.similarity_weight * similarity_score
            )
            scored.append((candidate, final_score))

        scored.sort(key=lambda pair: pair[1], reverse=True)

        top_candidates = scored[: max(0, top_k)] if top_k > 0 else scored
        return [(c.text, round(score, 4)) for c, score in top_candidates]

    def rank_answer_candidates(
        self,
        questions: Sequence[str],
        top_k: int = 3,
        min_keywords_to_consider: int = 0,
    ) -> List[List[Tuple[str, float]]]:
        """Return a list for each question containing (answer_text, score) tuples."""
        if not questions:
            return []
        return [
            self.rank_answer_candidates_for_question(q, top_k=top_k, min_keywords_to_consider=min_keywords_to_consider)
            for q in questions
        ]

    @staticmethod
    def _keyword_overlap_score(question_tokens: Set[str], answer_keywords: Set[str]) -> float:
        if not answer_keywords:
            return 0.0
        if not question_tokens:
            return 0.0

        matched = 0
        for keyword in answer_keywords:
            if keyword in question_tokens:
                matched += 1
                continue
            # Allow for very close matches for hyphenation/casing variants
            for token in question_tokens:
                if ESGAnswerRanker._string_similarity(keyword, token) >= 0.92:
                    matched += 1
                    break

        return matched / float(len(answer_keywords))

    @staticmethod
    def _string_similarity(a: str, b: str) -> float:
        if not a or not b:
            return 0.0
        # difflib returns [0, 1]
        return SequenceMatcher(a=a.lower(), b=b.lower()).ratio()

    @staticmethod
    def _load_candidates_from_json(qa_pairs_path: str) -> List[AnswerCandidate]:
        if not os.path.exists(qa_pairs_path):
            raise FileNotFoundError(f"qa_pairs.json not found at: {qa_pairs_path}")
        with open(qa_pairs_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # Support either {"qa_pairs": [...]} or a top-level list
        items = data.get("qa_pairs") if isinstance(data, dict) else data
        if not isinstance(items, list):
            raise ValueError("qa_pairs.json must be a list or contain a 'qa_pairs' list")

        candidates: List[AnswerCandidate] = []
        for idx, obj in enumerate(items):
            if not isinstance(obj, dict):
                continue
            q_text = (obj.get("question") or "").strip()
            a_text = (obj.get("answer") or "").strip()
            if not a_text:
                continue
            # Use the source question tokens as high-signal keywords for matching
            q_tokens = _tokenize(q_text) - _STOPWORDS
            # Include a small subset of answer tokens to enrich matching if needed
            a_tokens = {t for t in _tokenize(a_text) if len(t) >= 5} - _STOPWORDS
            # Limit answer-derived tokens to avoid noise
            enriched_keywords = set(list(q_tokens)[:20]) | set(list(a_tokens)[:10])
            label = q_text[:80] or f"Answer {idx+1}"
            candidates.append(_build_candidate(label=label, text=a_text, keywords=enriched_keywords))

        return candidates


__all__ = [
    "ESGAnswerRanker",
    "AnswerCandidate",
]

